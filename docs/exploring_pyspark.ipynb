{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faff6f6",
   "metadata": {},
   "source": [
    "# Exploring Pyspark\n",
    "Exploring Pyspark with NYC Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d37a37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cdefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b7c8c",
   "metadata": {},
   "source": [
    "Pyspark will not run if Java is not installed on the computer. PySpark 4.x requires Java 17 or 21. PySpark 3.5 works perfectly with Java 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecd34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip uninstall pyspark -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9669d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force install pyspark version\n",
    "#!pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79989e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ef9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr,col, regexp_replace, coalesce, lit, when\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Pyspark Session\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file in pyspark\n",
    "df = spark.read.csv(\n",
    "    r'C:\\Users\\User\\Documents\\PORTFOLIO\\NYC TAXI DATA\\yellow_tripdata\\csv_trip_data\\yellow_tripdata_2024-01.csv'\n",
    "    ,header=True #sets the first row to be the header\n",
    "    ,inferSchema = True #Default is string if infer Schema is not specified.Ensures to infer the correct datatype of columns. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515be955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to read files\n",
    "df=spark.read.option('header','true').csv(\n",
    "    r'C:\\Users\\User\\Documents\\PORTFOLIO\\NYC TAXI DATA\\yellow_tripdata\\csv_trip_data\\yellow_tripdata_2024-01.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe type\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42028a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns in pyspark\n",
    "df.select('VendorID','tpep_pickup_datetime').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca22987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Parquet file\n",
    "df_parq= spark.read.parquet(\n",
    "    r'C:\\Users\\User\\Documents\\PORTFOLIO\\NYC TAXI DATA\\yellow_tripdata\\yellow_tripdata_2024-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a615308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns\n",
    "df_parq.select('VendorID','tpep_pickup_datetime','store_and_fwd_flag').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.select('store_and_fwd_flag').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a195cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.dtypes\n",
    "df_parq.select('VendorID','tpep_pickup_datetime').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e3917",
   "metadata": {},
   "source": [
    "# Transformation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fcba8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add column\n",
    "# A simple add is df.withColumn('New Column Name',derived column_value).col references a dataframe column that is called\n",
    "df_parq.withColumn(\"Vendor_name\",\n",
    "    when(col(\"VendorID\") == 1, \"Creative Mobile Technologies, LLC\")\n",
    "    .when(col(\"VendorID\") == 2, \"Curb Mobility, LLC\")\n",
    "    .when(col(\"VendorID\") == 6, \"Myle Technologies Inc\")\n",
    "    .when(col(\"VendorID\") == 7, \"Helix\")\n",
    "    .otherwise(\"No Vendor\")\n",
    ")\\\n",
    ".withColumn(\n",
    "        \"store_and_forward_trip_flag\",\n",
    "        coalesce(regexp_replace(col(\"store_and_fwd_flag\"), '\"', ''), lit(\"N/A\"))\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sql case when inside Pyspark\n",
    "\n",
    "## Add additional columns. To add multiple columns just continue adding .withColumn after each column. \\ specifies a new line\n",
    "\n",
    "df_parq = df_parq\\\n",
    ".withColumn(\n",
    "    \"vendor_name\",\n",
    "    expr(\"\"\"\n",
    "        case\n",
    "            when vendorID = 1 then 'Creative Mobile Technologies, LLC'\n",
    "            when VendorID = 2 then 'Curb Mobility, LLC'\n",
    "            when VendorID = 6 then 'Myle Technologies Inc'\n",
    "            when VendorID = 7 then 'Helix'\n",
    "            else 'No Vendor'\n",
    "        end\n",
    "    \"\"\")\n",
    ")\\\n",
    ".withColumn(\n",
    "    \"rate_code_type\",\n",
    "    expr(\"\"\"\n",
    "        case \n",
    "            when ratecodeid = 1 then 'Standard rate'\n",
    "            when ratecodeid = 2 then 'JFK'\n",
    "            when ratecodeid = 3 then 'Newark'\n",
    "            when ratecodeid = 4 then 'Nassau or Westchester'\n",
    "            when ratecodeid = 5 then 'Negotiated fare'\n",
    "            when ratecodeid = 6 then 'Group ride'\n",
    "            when ratecodeid = 99 then 'Unknown'\n",
    "            else 'N/A'\n",
    "        end\n",
    "    \"\"\")\n",
    ")\\\n",
    ".withColumn(\n",
    "    \"payment_type\", #since column name already exists, it overwrites it\n",
    "    expr(\"\"\"\n",
    "        case\n",
    "            when payment_type = 0 then 'Flex Fare trip'\n",
    "            when payment_type = 1 then 'Credit card'\n",
    "            when payment_type = 2 then 'Cash'\n",
    "            when payment_type = 3 then 'No charge'\n",
    "            when payment_type = 4 then 'Dispute'\n",
    "            when payment_type = 5 then 'Unknown'\n",
    "            when payment_type = 6 then 'Voided trip'\n",
    "            else 'N/A'\n",
    "        end\n",
    "    \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add forgotten column\n",
    "df_parq = df_parq\\\n",
    ".withColumn(\"trip_duration_mins\", \n",
    "            expr(\"(unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 60\")\n",
    ")\\\n",
    ".withColumn(\"store_and_forward_trip_flag\",\n",
    "            expr(\"coalesce(regexp_replace(store_and_fwd_flag, '\\\"', ''), 'N/A')\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a98ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_parq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ae246",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop Columns\n",
    "df_parq=df_parq.drop('store_and_fwd_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "#df_parq.withColumnRenamed('Airport_fee','airport_fee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Rename multiple columns\n",
    "df_parq=df_parq\\\n",
    "    .withColumnRenamed('VendorID','vendor_id').withColumnRenamed('RatecodeID','rate_code_id')\\\n",
    "    .withColumnRenamed('tpep_pickup_datetime','pickup_time').withColumnRenamed('tpep_dropoff_datetime','dropoff_time')\\\n",
    "    .withColumnRenamed('DOLocationID','droppoff_zone').withColumnRenamed('PULocationID','pickup_zone')\\\n",
    "    .withColumnRenamed('Airport_fee','airport_fee').withColumnRenamed('extra','extra_fees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6198e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90891c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "cols = [\n",
    "        \"vendor_id\",\"vendor_name\",\"pickup_time\",\"dropoff_time\",\"trip_duration_mins\",\"passenger_count\",\"trip_distance\"\n",
    "        ,\"rate_code_id\",\"rate_code_type\",\"store_and_forward_trip_flag\",\"pickup_zone\",\"droppoff_zone\",\"payment_type\"\n",
    "        ,\"fare_amount\",\"extra_fees\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\"\n",
    "        ,\"congestion_surcharge\",\"airport_fee\"\n",
    "       ]\n",
    "df_parq= df_parq.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter can be done both ways\n",
    "\n",
    "#df_parq.filter('trip_distance <= 0').show()\n",
    "df_parq.filter(df_parq['trip_distance'] <= 0).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only necessary rows\n",
    "df_parq.filter('trip_distance <= 0').select(['vendor_name','trip_distance']).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple filter operations --& for and | for or \n",
    "df_parq.filter( (df_parq['trip_distance'] <= 0) & (df_parq['tip_amount'] > 0)  ).\\\n",
    "select('vendor_name','pickup_time','dropoff_time','trip_distance','tip_amount','fare_amount','payment_type').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88aca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d583107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invalid trip data to be stored for data quality checks\n",
    "df_invalid_tripdata = df_parq.filter((df_parq['trip_distance'] <= 0) | (df_parq['fare_amount'] <= 0) | (df_parq['passenger_count'] <= 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a241f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalid_tripdata.select('vendor_name','pickup_time','dropoff_time','trip_distance','tip_amount','fare_amount','passenger_count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71657f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 to exclude invalid trip data and only work with valid trips. This performs set difference, doesn't exclude nulls, expensive and not best practice for ETL purpose\n",
    "#df_clean1=df_parq.subtract(df_invalid_tripdata) #remove invalid data from whole dataset \n",
    "\n",
    "# Option 2 filters only what is needed and handles null properly\n",
    "df_clean=df_parq.filter( (df_parq['trip_distance'] > 0) & (df_parq['fare_amount'] > 0) & (df_parq['passenger_count'] > 0) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation check \n",
    "df_clean.filter((df_parq['trip_distance'] <= 0) | (df_parq['fare_amount'] <= 0) | (df_parq['passenger_count'] <= 0) )\\\n",
    ".select('vendor_name','pickup_time','dropoff_time','trip_distance','fare_amount','passenger_count').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag anomalies and log into another dataframe\n",
    "df_clean.filter((df_parq['trip_duration_mins'] > 180) & (df_parq['trip_distance'] > 100) & (df_parq['fare_amount'] > 500))\\\n",
    ".select('vendor_name','pickup_time','dropoff_time','trip_distance','fare_amount','trip_duration_mins').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25794bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, when, col, size, lit\n",
    "\n",
    "anomalies_data = df_clean.withColumn(\n",
    "    \"anomaly_type\",\n",
    "    array(\n",
    "        when(col(\"fare_amount\") > 500, lit(\"high_fare\")) ,\n",
    "        when(col(\"trip_distance\") > 100, lit(\"long_distance\")),\n",
    "        when(col(\"trip_duration_mins\") > 180,lit( \"long_duration\")),\n",
    "        when(col(\"passenger_count\") > 6, lit(\"high_passenger_count\"))\n",
    "    ) #returns an array of the type of anomaly for each condition\n",
    ").withColumn(\"anomaly_type\",\n",
    "    expr(\"filter(anomaly_type, x -> x is not null)\") #removes null anomaly to focus on actual anomalies\n",
    ").filter(\n",
    "    size(col(\"anomaly_type\")) > 0 #only retains those with anomaly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_data.filter(size(col(\"anomaly_type\")) > 1).select('vendor_name','trip_distance','fare_amount','trip_duration_mins','anomaly_type').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f22d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc870d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084edd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c0550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd221fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcc520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f04e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875d991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683672cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d074c6c",
   "metadata": {},
   "source": [
    "## Other Transformation functions for knowledge base ...Not required for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows\n",
    "#df_parq.select('trip_distance').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43adf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.na.drop(how=\"any\",thresh=2,subset=['columnname']).show()\n",
    "parameters inside drop()\n",
    "how - can accept 2 values:\n",
    "    any - drop rows with any values as null\n",
    "    all - drop rows with all values as null\n",
    "thresh - specifies the threshold of non null values required to be present\n",
    "    if thresh is set to 2, it deletes rows where less than 2 non null values appear\n",
    "subset - deletes null values in the specified column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d448fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.selectExpr(\n",
    "    *[f\"count({c}) as {c}\" for c in df_parq.columns]\n",
    ").show() #count values in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq.filter(col(\"trip_distance\").isNull()).show(5)\n",
    "#df_parq.filter(col(\"passenger_count\").isNull()).count() #count null values in column\n",
    "#df_drop_test = df_parq.na.drop(how=\"any\",thresh=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475648cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_drop_test.filter(col(\"passenger_count\").isNull()).count()\n",
    "df_drop_test.selectExpr(\n",
    "    *[f\"count({c}) as {c}\" for c in df_drop_test.columns]\n",
    ").show() #count values in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af503986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values\n",
    "#df.na.fill('updatedvalue',subset)\n",
    "df_parq.select('passenger_count').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_parq.na.fill('Unknown','passenger_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('passenger_count').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717a283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a59f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642c91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
